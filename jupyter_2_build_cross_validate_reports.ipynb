{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9a963-097d-4f7c-a92d-547cf7607682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "format_log = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(filename='build_cross_validate_reports.log', format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.DEBUG, datefmt='%m/%d/%Y %I:%M:%S %p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a86a7-3aba-4f53-8a4d-3e1f5ace1e26",
   "metadata": {},
   "source": [
    "# Build Reports for Dialogflow K-fold Cross Validation\n",
    "\n",
    "### Overview\n",
    "This is Jupyter Notebook to build reports to analyze the results of Dialogflow k-fold cross validation in the web app and Excel\n",
    "\n",
    "### Notebook Steps\n",
    "1. Loads the current and previous k-fold cross validation data\n",
    "2. Builds dataframes that will be used to output the reports\n",
    "3. Builds and combines the previous k-fold reports for comparision\n",
    "4. Allows flags and thresholds to be added to the report.\n",
    "5. Creates a confusion matrix of prediction\n",
    "6. Outputs the data to Excel for analysis and the web app to analyze intents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3749b1-2b9f-40bc-9fb8-32bd4970a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "logging.info(\"building reports for k-fold cross-validation started\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import date\n",
    "import pathlib\n",
    "\n",
    "from config.definitions import ROOT_DIR\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n",
    "#from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from pandas import DataFrame\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "             \n",
    "logging.info(\"Python libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96430edd-e449-48b9-bfff-a8b3faef0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders\n",
    "process_path1 = os.path.join('data','processed',\"\")\n",
    "\n",
    "isExist = os.path.exists(process_path1)\n",
    "if isExist == False:\n",
    "    create_path = pathlib.Path(process_path1)\n",
    "    create_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "# create folders\n",
    "process_path2 = os.path.join('data','output',\"\")\n",
    "\n",
    "isExist = os.path.exists(process_path2)\n",
    "if isExist == False:\n",
    "    create_path = pathlib.Path(process_path2)\n",
    "    create_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8712bf0-39de-4bd0-a275-8ad8fa2b7c09",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "Load actual vs predicted results into the processed folder before continuing\n",
    "+ Column Names:text, actual_intent, pred_intent, pred_conf\n",
    "+ File Type: Excel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76779c6c-ffc9-41be-80ef-c41dbd524cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Values\n",
    "\n",
    "# path to load current and previous files\n",
    "# defaults to getting the last two recent files. If you want to change the files can manually enter them here.\n",
    "\n",
    "k_fold_files = os.listdir(process_path1)\n",
    "k_fold_files.sort()\n",
    "\n",
    "current_file_path = process_path1\n",
    "current_file_name = 'pwc_2022_06_29.xlsx'\n",
    "#current_file_name = k_fold_files[-1]\n",
    "\n",
    "prev_file_path = process_path1\n",
    "prev_file_name = 'pwc_2022_06_29.xlsx'\n",
    "#prev_file_name = k_fold_files[-2]\n",
    "\n",
    "# intents to ignore for the report\n",
    "intents_to_ignore = ['Default Fallback Intent', 'Default Welcome Intent']\n",
    "\n",
    "# threshold values for flags\n",
    "support_less_than = 30\n",
    "f1_less_than = .67\n",
    "wrong_prediction_percent_greater_than = .30\n",
    "class_imbalance_ratio_greater_than = 20\n",
    "\n",
    "logging.info(\"Parameters initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4895e2d-e912-44c6-a5b9-78212c03f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build tables for analysis\n",
    "\n",
    "def create_analysis_tables(dataframe_input_list):\n",
    "\n",
    "    '''\n",
    "    Overivew: A function to build tables that will be used for the analysis\n",
    "    Depends On Function: None\n",
    "    Constraints: You must have panda dataframes in a list. Each panda dataframe should be formatted:\n",
    "        text (str), acutal_intent(str), pred_intent (str), pred_conf (float), test_num (int)\n",
    "    Input:\n",
    "        1. A list of panda dataframes (list)\n",
    "    Output:\n",
    "        1. df_intents (dataframe):\n",
    "        2. df_conf_mat (dataframe):\n",
    "        3. df_summary (dataframe):\n",
    "        4. df_plot (dataframe):\n",
    "        5: df_plot_wrong_grouped (dataframe): \n",
    "    '''\n",
    "    \n",
    "    if type(dataframe_input_list) == list:\n",
    "        all_data  = pd.concat(dataframe_input_list)\n",
    "        df_intents = all_data[['text','actual_intent','pred_intent', 'pred_conf']]\n",
    "    else:\n",
    "        all_data = dataframe_input_list\n",
    "        df_intents = dataframe_input_list\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    df_intents is table that is tidy data of the test data results\n",
    "    Ex: This phrase, was supposed to go to this intent, but went to this this intent instead with this confident score from dialogflow.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    df_conf_mat is table that gives the number of times a predicted intent occured for an actual intent\n",
    "    Ex: The predicted intent cme.3.1-callme_cancel got predicted five times for the actual intent !cancel_intent\n",
    "    '''\n",
    "    df_conf_mat = (df_intents\n",
    "      .groupby([\"actual_intent\", \"pred_intent\"])\n",
    "      .agg(n_pred=(\"pred_conf\", \"size\"))\n",
    "      .reset_index())\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    df_summary is a table that gives the number of predictions made for an intent\n",
    "    Ex: !cancel_intent got predicted 50 times in the test data\n",
    "    '''\n",
    "    df_summary = (df_intents\n",
    "     .groupby('pred_intent')\n",
    "     .agg(n=('pred_conf', 'size'),\n",
    "          mean_conf=('pred_conf', 'mean')))\n",
    "    df_summary = df_summary.rename(columns={'n':'predictions', 'mean_conf':'df_conf'})\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    df_plot is a table that combines the tables of df_conf_mat and df_summary into one table.\n",
    "    So you can see how many times dilogflow predicted an intent for certain intent and the overall number of times that intent was predicted.\n",
    "    Ex: Dialogflow predicted cme.3.1-callme_cancel five times for the actual intent !cancel_intent. Overall cme.3.1-callme_cancel was\n",
    "        predicted forty-three times by Dialogflow. Of those forty-three predictions, P means it predicted cme.3.1-callme_cancel 11.6% of those\n",
    "        43 predictions for the intent !cancel_intent. \n",
    "\n",
    "    This helps to understand the distribution of the predictions and where model predicted incorrectly or correctly often.\n",
    "    '''\n",
    "    df_plot = df_conf_mat.merge(df_summary.reset_index()).assign(p=lambda d: d['n_pred']/d['predictions'])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    df_plot_wrong_grouped is a table that counts how many times dialogflow made a prediction, that prediction was incorrect.\n",
    "    Ex: !cancel_intent was predicted 50, of those 50 times it was predicted, it was predicted wrong 50 times.\n",
    "    '''\n",
    "    df_plot['correct_flag'] = np.where(df_plot['actual_intent'] == df_plot['pred_intent'], 1,0)\n",
    "    df_plot_wrong = df_plot[df_plot['correct_flag'] == 0]\n",
    "    df_plot_wrong_grouped = df_plot_wrong.groupby('pred_intent')['n_pred'].sum().reset_index().set_index('pred_intent')\n",
    "    df_plot_wrong_grouped = df_plot_wrong_grouped.rename(columns={'n_pred':'FP'})\n",
    "     \n",
    "    return all_data, df_intents, df_conf_mat, df_summary, df_plot, df_plot_wrong_grouped\n",
    "    #incorrectly_predicted = FP\n",
    "\n",
    "def compute_overall_f1_score(dataframe, ignore_intents=[]):\n",
    "    '''\n",
    "    **THIS IS CURRENTLY NOT USED, BUT HERE FUTURE USE IN CASE EVER NEEDED**\n",
    "    The is a fuction to computer the overall F1-score and class balance score using the macro, \n",
    "    micro and weighted to create an average score. \n",
    "    '''\n",
    "    \n",
    "    dataframe = dataframe[~dataframe.index.isin(ignore_intents)]\n",
    "    test_y = dataframe['actual_intent'].values\n",
    "    preds = dataframe['pred_intent'].values\n",
    "    \n",
    "    macro_f1 = f1_score(test_y, preds, average='macro', labels=np.unique(preds))\n",
    "    micro_f1 = f1_score(test_y, preds, average='micro', labels=np.unique(preds))\n",
    "    weighted_f1 = f1_score(test_y, preds, average='weighted', labels=np.unique(preds))\n",
    "\n",
    "    combined_avg = (macro_f1+micro_f1+weighted_f1)/3\n",
    "    combined_avg = round(combined_avg,4) * 100\n",
    "    \n",
    "    balance_score = balanced_accuracy_score(test_y, preds, adjusted=True)\n",
    "    return combined_avg, balance_score\n",
    "\n",
    "\n",
    "def built_report(all_dataframe, summary_dataframe, wrong_group_dataframe, confusion_matrix, ignore_intents=[]):\n",
    "    '''\n",
    "    This is a function to combine dataframes for each k-fold data\n",
    "    '''\n",
    "    test_y = all_dataframe['actual_intent'].values\n",
    "    preds = all_dataframe['pred_intent'].values\n",
    "    report_dict = classification_report(test_y, preds, output_dict=True, labels=np.unique(preds))\n",
    "    result_df = pd.DataFrame(report_dict).transpose()\n",
    "    \n",
    "    # removes the last three summary f1 scores. Uncomment out to add back, but will mess up downstream reports\n",
    "    result_df = result_df.iloc[:-3]\n",
    "    \n",
    "    result_df = result_df.merge(summary_dataframe, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    #add false positive and false negative counts to intents\n",
    "    fp_df = confusion_matrix['Actual'].transpose()\n",
    "    fp_s = fp_df['FP Total']\n",
    "\n",
    "    fn_df = confusion_matrix['Actual']\n",
    "    fn_s = fn_df['FN Total']\n",
    "    \n",
    "    result_df = result_df.merge(fp_s, how='left', left_index=True, right_index=True)\n",
    "    result_df = result_df.merge(fn_s, how='left', left_index=True, right_index=True)\n",
    "    result_df = result_df.rename(columns={'FP Total':'fp', 'FN Total':'fn'})\n",
    "    \n",
    "    #ignore intents\n",
    "    result_df = result_df[~result_df.index.isin(ignore_intents)]\n",
    "    \n",
    "    # add calculations\n",
    "    result_df['fp_%'] = (result_df['fp'] / result_df['predictions'])\n",
    "    result_df['fn_%'] = (result_df['fn'] / result_df['predictions'])\n",
    "    result_df['prec_recall_delta'] = abs(result_df['precision'] - result_df['recall'])\n",
    "    result_df['f1_df_predict_delta'] = abs(result_df['f1-score'] - result_df['df_conf'])\n",
    "    result_df['support_ratio'] = result_df['support'].max() / result_df['support']\n",
    "    return result_df\n",
    "\n",
    "def build_history_report(current_dataframe, previous_dataframe):\n",
    "    '''\n",
    "    This is function that combines the build_report function to help compare the previous k-fold report to the current \n",
    "    k-fold report.\n",
    "    '''\n",
    "    # merge the two reports\n",
    "    history_df = current_report.merge(previous_report, how='left', left_index=True, right_index=True, suffixes=('_cur', '_prev'))\n",
    "    \n",
    "    # compute differences\n",
    "    history_df['precision_diff'] = history_df['precision_cur'] - history_df['precision_prev']\n",
    "    history_df['recall_diff'] = history_df['recall_cur'] - history_df['recall_prev']\n",
    "    history_df['f1-score_diff'] = history_df['f1-score_cur'] - history_df['f1-score_prev']\n",
    "    history_df['df_conf_diff'] = history_df['df_conf_cur'] - history_df['df_conf_prev']\n",
    "    history_df['fp_diff'] = history_df['fn_cur'] - history_df['fn_prev']\n",
    "    history_df['fp_%_diff'] = history_df['fn_%_cur'] - history_df['fn_%_prev']\n",
    "    history_df['fn_diff'] = history_df['fn_cur'] - history_df['fn_prev']\n",
    "    history_df['fn_%_diff'] = history_df['fn_%_cur'] - history_df['fn_%_prev']\n",
    "    \n",
    "    # rename columns\n",
    "    rename_dict = {'support_cur':'support',\n",
    "                   'support_ratio_cur':'support_ratio'\n",
    "                  }\n",
    "    history_df = history_df.rename(columns=rename_dict)\n",
    "    \n",
    "    #organize the columns\n",
    "    cols = ['support','support_ratio','predictions_cur','fp_cur','fp_prev','fp_diff','fn_cur','fn_prev','fn_diff','precision_cur','precision_prev','precision_diff','recall_cur','recall_prev','recall_diff','f1-score_cur','f1-score_prev','f1-score_diff',\n",
    "        'df_conf_cur','df_conf_prev','df_conf_diff','fp_%_cur','fp_%_prev','fp_%_diff', 'fn_%_cur','fn_%_prev','fn_%_diff']\n",
    "    \n",
    "    history_df = history_df[cols]\n",
    "    return history_df\n",
    "\n",
    "def add_flags(dataframe, support_flag_value, f1_flag_value, wrong_pred_flag_value, imbalance_ratio_flag):\n",
    "    '''\n",
    "    This is a function to add flags to the history dataframe \n",
    "    **NOTE**: can only be run once otherwise duplicates flag total values\n",
    "    '''\n",
    "    support_less_than = support_flag_value\n",
    "    f1_less_than = f1_flag_value\n",
    "    wrong_prediction_percent_greater_than = wrong_pred_flag_value\n",
    "    \n",
    "    # intents that need more training examples low support intents\n",
    "    dataframe['support_flag'] = np.where(dataframe['support'] < support_less_than, 1,0)\n",
    "\n",
    "    # intents with support greater than 30 training samples, but f1 lower than threshold\n",
    "    dataframe['f1_flag'] = np.where( (dataframe['support'] >= support_less_than) & (dataframe['f1-score_cur'] < f1_less_than), 1,0)\n",
    "\n",
    "    # intents with support greater than 30 training samples and difference between f1-score and dialogflow confidence meets threshold\n",
    "    #dataframe['wrong_predict_flag'] = np.where( (dataframe['support'] >= support_less_than) & (dataframe['incorrectly_predicted_cur'] > wrong_prediction_percent_greater_than), 1,0)\n",
    "    \n",
    "    # class imbalance flag\n",
    "    dataframe['class_imbalance_flag'] = np.where( dataframe['support_ratio'] >= imbalance_ratio_flag, 1,0)\n",
    "\n",
    "    # intents with support greater than 30 training samples and difference between f1-score and dialogflow confidence meets threshold\n",
    "    dataframe['flag_total'] = dataframe.iloc[:, -3:].sum(axis=1)\n",
    "\n",
    "    cols_to_add = ['support_flag', 'f1_flag','flag_total']\n",
    "    flag_totals = dataframe[cols_to_add].transpose().iloc[:, :].sum(axis=1).reset_index().rename(columns={'index':'flag',0:'total'})\n",
    "    return dataframe, flag_totals\n",
    "\n",
    "\n",
    "def build_prediction_matrix(k_fold_data,  ignore_intents=[]):\n",
    "    '''\n",
    "    Overview: A function to create a confusion matrix from k-fold cross-validation data\n",
    "    Input: \n",
    "        1. k_fold_data (dataframe): The Dialogflow k-fold test data\n",
    "    Output:\n",
    "        1. prediction_maxtrix_df (dataframe): The confusion matrix\n",
    "    '''\n",
    "    \n",
    "    y_true = k_fold_data['actual_intent'].values\n",
    "    y_pred = k_fold_data['pred_intent'].values\n",
    "    labels = [x for x in np.unique(y_pred)]\n",
    "    cf_m = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    df_cm = DataFrame(cf_m, index=labels, columns=labels)\n",
    "    df_cm = df_cm[~df_cm.index.isin(ignore_intents)]\n",
    "    prediction_maxtrix_df = df_cm.reset_index()\n",
    "    return prediction_maxtrix_df\n",
    "    \n",
    "    \n",
    "def build_total_matrix(confusion_matrix_df):\n",
    "    '''\n",
    "    Overview: A function to output a confusion matrix with totals and multi-index columns for output\n",
    "    Input:\n",
    "        1. confusion_matrix_df (dataframe): A standard confusion matrix created from sklearn.\n",
    "    Output:\n",
    "        1. matrix_totals (dataframe)\n",
    "    '''\n",
    "    matrix_totals = confusion_matrix_df\n",
    "    matrix_totals = matrix_totals.rename(columns={'index':'Expected'})\n",
    "    matrix_totals.loc[:,'Expected Total'] = matrix_totals.sum(axis=1)\n",
    "    header = pd.MultiIndex.from_product([['Actual'],list(matrix_totals.columns)])\n",
    "    matrix_totals = pd.DataFrame(data=matrix_totals.iloc[:,1:].to_numpy(), index=matrix_totals['Expected'], columns = header[1:])\n",
    "    matrix_totals.loc['Actual Total',:]= matrix_totals.sum(axis=0)\n",
    "    return matrix_totals\n",
    "\n",
    "def build_true_false_matrix(confusion_matrix_df):\n",
    "    '''\n",
    "    Overview: A function to output a confusion matrix with totals and multi-index columns for output\n",
    "    Input:\n",
    "        1. confusion_matrix_df (dataframe): A standard confusion matrix created from sklearn.\n",
    "    Output:\n",
    "        1. matrix_totals (dataframe)\n",
    "    '''\n",
    "    matrix_totals = confusion_matrix_df\n",
    "    matrix_totals = matrix_totals.set_index('index')\n",
    "    np.fill_diagonal(matrix_totals.values, 0)\n",
    "    matrix_totals = matrix_totals.reset_index(drop=False)\n",
    "    matrix_totals = matrix_totals.rename(columns={'index':'Expected'})\n",
    "    matrix_totals.loc[:,'FN Total'] = matrix_totals.sum(axis=1)\n",
    "    header = pd.MultiIndex.from_product([['Actual'],list(matrix_totals.columns)])\n",
    "    matrix_totals = pd.DataFrame(data=matrix_totals.iloc[:,1:].to_numpy(), index=matrix_totals['Expected'], columns = header[1:])\n",
    "    matrix_totals.loc['FP Total',:]= matrix_totals.sum(axis=0)\n",
    "    return matrix_totals\n",
    "\n",
    "\n",
    "logging.info(\"Functions created\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da695e-2742-4dbd-8b26-500236c37eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files\n",
    "#current_df = pd.read_pickle(current_file_path+current_file_name)\n",
    "#prev_df = pd.read_pickle(prev_file_path+prev_file_name)\n",
    "\n",
    "current_df = pd.read_excel(current_file_path+current_file_name)\n",
    "prev_df = pd.read_excel(prev_file_path+prev_file_name)\n",
    "\n",
    "logging.info(\"Current and previous files loaded\")\n",
    "#current_df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc020783-9176-4bf0-8895-211b5cf66a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the functions in create_analysis_tables to build dataframes for the report\n",
    "\n",
    "cur_all_df, cur_intents_df, cur_conf_mat_df, cur_summ_df, cur_plot_df, cur_plot_wrong_grouped_df = create_analysis_tables(current_df)\n",
    "prev_all_df,prev_intents_df, prev_conf_mat_df, prev_summ_df, prev_plot_df, prev_plot_wrong_grouped_df = create_analysis_tables(prev_df)\n",
    "\n",
    "logging.info(\"Current and previous tables for analysis created\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68877670-fe46-4153-95e6-b2e3cfa3cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build confusion matrices for current data\n",
    "\n",
    "cur_prediction_matrix = build_prediction_matrix(cur_all_df)\n",
    "cur_prediction_matrix_total = build_total_matrix(cur_prediction_matrix)\n",
    "cur_prediction_matrix_zeroed = build_true_false_matrix(cur_prediction_matrix)\n",
    "#cur_prediction_matrix_zeroed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda8dd5-369f-4dea-8562-97d65198a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build confusion matrices for previous data\n",
    "\n",
    "prev_prediction_matrix = build_prediction_matrix(prev_all_df)\n",
    "prev_prediction_matrix_total = build_total_matrix(prev_prediction_matrix)\n",
    "prev_prediction_matrix_zeroed = build_true_false_matrix(prev_prediction_matrix)\n",
    "#prev_prediction_matrix_zeroed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649746c3-61cc-4d92-a616-451d10b8e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track deleted and added phrases\n",
    "prev_all_df['index'] = prev_all_df['text']+ prev_all_df['actual_intent']\n",
    "cur_all_df['index'] = cur_all_df['text']+ cur_all_df['actual_intent']\n",
    "\n",
    "prev_all_df = prev_all_df.set_index('index')\n",
    "cur_all_df = cur_all_df.set_index('index')\n",
    "merged = prev_all_df.merge(cur_all_df, how='outer', left_index=True, right_index=True, suffixes=('_prev', '_cur'), indicator=True)\n",
    "\n",
    "#phrases added\n",
    "cols = ['text_cur','actual_intent_cur']\n",
    "phrases_added = merged[cols][merged['_merge']=='right_only'].reset_index(drop=True)\n",
    "phrases_added = phrases_added.rename(columns={'text_cur':'deleted text', 'actual_intent_cur':'intent'})\n",
    "\n",
    "#phrases deleted or changed\n",
    "cols = ['text_prev', 'actual_intent_prev'] \n",
    "phrases_deleted_or_changed = merged[cols][merged['_merge']=='left_only'].reset_index(drop=True)\n",
    "phrases_deleted_or_changed = phrases_deleted_or_changed.rename(columns={'text_prev':'added text', 'actual_intent_prev':'intent'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18a2b9-1f55-40bf-af82-f58d312bd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds the report for the previous and current data\n",
    "\n",
    "ignore_intents = []\n",
    "#ignore_intents = intents_to_ignore\n",
    "#ignore_knowledge_intents_list = list(set(cur_all_df[cur_all_df['pred_intent'].str.startswith('Knowledge.')==True]['pred_intent'])) #ignoring the knowledge intents\n",
    "#ignore_intents = ignore_knowledge_intents_list + ignore_intents\n",
    "\n",
    "current_report = built_report(cur_all_df, cur_summ_df, cur_plot_wrong_grouped_df, cur_prediction_matrix_zeroed, ignore_intents=ignore_intents)\n",
    "previous_report = built_report(prev_all_df, prev_summ_df, prev_plot_wrong_grouped_df, prev_prediction_matrix_zeroed, ignore_intents=ignore_intents)\n",
    "\n",
    "logging.info(\"Current and previous reports created\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5adf3-2ab1-43b8-9701-f6b0b1f49f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines the previous and current data into one dataframe for comparison\n",
    "\n",
    "history_df = build_history_report(current_report, previous_report)\n",
    "\n",
    "logging.info(\"History table created\")\n",
    "#history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708aa663-02af-4573-8d2b-e4990f8aabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buidling the summary table that will be outputed to compare the previous and current data\n",
    "\n",
    "current_f1 = history_df['f1-score_cur'].mean()\n",
    "previous_f1 = history_df['f1-score_prev'].mean()\n",
    "f1_difference = current_f1 - previous_f1\n",
    "\n",
    "current_total_fp = history_df['fp_cur'].sum()\n",
    "previous_total_fp = history_df['fp_prev'].sum()\n",
    "fp_difference = current_total_fp - previous_total_fp\n",
    "\n",
    "current_total_fn = history_df['fn_cur'].sum()\n",
    "previous_total_fn = history_df['fn_prev'].sum()\n",
    "fn_difference = current_total_fn - previous_total_fn\n",
    "\n",
    "\n",
    "current_balance = history_df['support_ratio'].mean()\n",
    "phrases_added_sum = phrases_added.shape[0]\n",
    "phrased_chngd_or_del_sum = phrases_deleted_or_changed.shape[0]\n",
    "\n",
    "summary_values = [current_f1, previous_f1, f1_difference, current_total_fp, previous_total_fp, fp_difference,\n",
    "                  current_total_fn, previous_total_fn, fn_difference, current_balance, phrases_added_sum, phrased_chngd_or_del_sum]\n",
    "\n",
    "index_vals = ['Current F1 Avg', 'Previous F1 Avg', 'F1 Diff',\n",
    "              'Current FP Total', 'Previous FP Total', 'FP Diff', \n",
    "              'Current FN Total', 'Previous FN Total', 'FN Diff',\n",
    "              'Current Class Imbalance Avg',\n",
    "             'Phrases Added or Changed', 'Phrases Removed or Changed']\n",
    "\n",
    "today = date.today()\n",
    "columns_vals = [today]\n",
    "\n",
    "f1_summary_df = pd.DataFrame(data=summary_values, columns=columns_vals, index=index_vals)\n",
    "\n",
    "logging.info(\"Summary table created\") \n",
    "#f1_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2b417-2a5d-41de-b652-4acd37df8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the descriptive stats table for output\n",
    "\n",
    "history_stats = history_df.describe()\n",
    "\n",
    "logging.info(\"Descriptive stats of history table created\") \n",
    "#history_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fec787-e4ba-4748-ac22-6008179cfff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the flags to the history dataframe\n",
    "\n",
    "flags_df, flag_total_df = add_flags(history_df, support_less_than, \n",
    "                                    f1_less_than, \n",
    "                                    wrong_prediction_percent_greater_than,\n",
    "                                   class_imbalance_ratio_greater_than)\n",
    "\n",
    "flags_df = flags_df.sort_values(by=['f1-score_cur','flag_total', ], ascending=[True,False])\n",
    "logging.info(\"Flags added to history table\") \n",
    "#flags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e640948-d64f-4907-8328-5b9bf4cfd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputing all the files to Excel\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "file_name = 'cv_scan'+now+'.xlsx'\n",
    "\n",
    "\n",
    "file_path = os.path.join('data','output','cross_validation_intent_level',\"\")\n",
    "\n",
    "#file_path = 'data/output/cross_validation_intent_level/'\n",
    "\n",
    "with pd.ExcelWriter(file_path+file_name) as writer: \n",
    "    f1_summary_df.to_excel(writer, sheet_name='overall_summary', index=True)\n",
    "    history_stats.to_excel(writer, sheet_name='descriptive_summary', index=True)\n",
    "    flags_df.to_excel(writer, sheet_name='intent_summary', index=True)\n",
    "    cur_prediction_matrix_total.to_excel(writer, sheet_name='prediction_matrix_totals', index=True)\n",
    "    cur_prediction_matrix_zeroed.to_excel(writer, sheet_name='FP_FN_matrix', index=True)\n",
    "    phrases_added.to_excel(writer, sheet_name='added_changed_phrases', index=False)\n",
    "    phrases_deleted_or_changed.to_excel(writer, sheet_name='deleted_changed_phrases', index=False)\n",
    "    cur_all_df.to_excel(writer, sheet_name='current_all_data', index=False)\n",
    "    prev_all_df.to_excel(writer, sheet_name='previous_all_data', index=False)\n",
    "    cur_prediction_matrix.to_excel(writer, sheet_name='prediction_matrix', index=False)\n",
    "    \n",
    "logging.info(\"Excel output file created\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb446ff-9873-4839-888c-98f1b55e62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"building reports for k-fold cross-validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7d74f-cf69-4e9e-8cac-4a4d3dafc541",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_prediction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba6316-c118-486f-852f-568f9cf3f18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
